{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import time\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transform to convert grayscale images to RGB\n",
    "class GrayscaleToRGB:\n",
    "    def __call__(self, img):\n",
    "        return img.convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FGSM attack implementation\n",
    "def fgsm_attack(model, data, target, epsilon):\n",
    "    data.requires_grad = True\n",
    "    output = model(data)\n",
    "    loss = criterion(output, target)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    data_grad = data.grad.data\n",
    "    perturbed_data = data + epsilon * data_grad.sign()\n",
    "    return perturbed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PGD attack\n",
    "def pgd_attack(model, data, target, epsilon, alpha, num_iter):\n",
    "    perturbed_data = data.clone().detach().requires_grad_(True).to(data.device)\n",
    "    for _ in range(num_iter):\n",
    "        output = model(perturbed_data)\n",
    "        loss = criterion(output, target)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        perturbed_data = perturbed_data + alpha * perturbed_data.grad.sign()\n",
    "        perturbed_data = torch.clamp(perturbed_data, data - epsilon, data + epsilon)\n",
    "        perturbed_data = torch.clamp(perturbed_data, 0, 1).detach().requires_grad_(True)\n",
    "    return perturbed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DeepFool attack\n",
    "def deepfool_attack(model, data, num_classes=10, overshoot=0.02, max_iter=50):\n",
    "    data = data.clone().detach().requires_grad_(True).to(data.device)\n",
    "    perturbed_data = data.clone().detach().requires_grad_(True).to(data.device)\n",
    "    output = model(perturbed_data)\n",
    "    _, pred = torch.max(output.data, 1)\n",
    "    for _ in range(max_iter):\n",
    "        output = model(perturbed_data)\n",
    "        loss = criterion(output, pred)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        grad = perturbed_data.grad.data\n",
    "        perturbed_data = perturbed_data + overshoot * grad.sign()\n",
    "        perturbed_data = torch.clamp(perturbed_data, 0, 1).detach().requires_grad_(True)\n",
    "        output = model(perturbed_data)\n",
    "        _, new_pred = torch.max(output.data, 1)\n",
    "        if not torch.equal(new_pred, pred):\n",
    "            break\n",
    "    return perturbed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CW attack\n",
    "def cw_attack(model, data, target, c=1e-4, kappa=0, max_iter=1000, learning_rate=0.01):\n",
    "    data = data.clone().detach().requires_grad_(True).to(data.device)\n",
    "    target = target.to(data.device)\n",
    "    optimizer = optim.Adam([data], lr=learning_rate)\n",
    "    for i in range(max_iter):\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            data = torch.clamp(data, 0, 1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(64 * 12 * 12 * 4, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LeNet-5 model\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define VGG16 model\n",
    "class VGG16(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.features = models.vgg16(pretrained=True).features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ResNet18 model\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DenseNet121 model\n",
    "class DenseNet121(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(DenseNet121, self).__init__()\n",
    "        self.model = models.densenet121(pretrained=True)\n",
    "        self.model.classifier = nn.Linear(self.model.classifier.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=28, hidden_size=128, num_layers=2, num_classes=10):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Transformer model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size=28, num_classes=10, d_model=128, nhead=8, num_layers=2):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_size, d_model)\n",
    "        self.pos_encoder = nn.Parameter(torch.zeros(1, 28, d_model))\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) + self.pos_encoder\n",
    "        x = x.permute(1, 0, 2)  # Transformer expects (seq_len, batch, d_model)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=0)  # Average over sequence length\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, interations, train_loader, optimizer, criterion, device, remove_channel_dim=False):\n",
    "    model.train()\n",
    "    for i in range(interations):\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            if remove_channel_dim:\n",
    "                data = data.squeeze(1)\n",
    "            if isinstance(model, LSTMModel) or isinstance(model, TransformerModel):\n",
    "                data = data.view(data.size(0), -1, data.size(-1))\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_dataset(transform_needed=False):\n",
    "    transform = transforms.Compose([\n",
    "        GrayscaleToRGB(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]) if transform_needed else transforms.ToTensor()\n",
    "    train_dataset = datasets.MNIST(root=\"data\", train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST(root=\"data\", train=False, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fashion_mnist_dataset(transform_needed=False):\n",
    "    transform = transforms.Compose([\n",
    "        GrayscaleToRGB(),\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]) if transform_needed else transforms.ToTensor()\n",
    "    train_dataset = datasets.FashionMNIST(root=\"data\", train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.FashionMNIST(root=\"data\", train=False, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_results_before_adversarial_attack(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            if isinstance(model, LSTMModel) or isinstance(model, TransformerModel):\n",
    "                data = data.squeeze(1)  \n",
    "                data = data.view(data.size(0), -1, data.size(-1)) \n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += target.size(0)\n",
    "    accuracy = correct / total\n",
    "    print(f'Accuracy before adversarial attack: {accuracy * 100:.2f}%')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_results_after_adversarial_attack(model, test_loader, attack_fn, attack_params, device):\n",
    "    if isinstance(model, LSTMModel) or isinstance(model, TransformerModel):\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        if isinstance(model, LSTMModel) or isinstance(model, TransformerModel):\n",
    "            data = data.squeeze(1) \n",
    "            data = data.view(data.size(0), -1, data.size(-1))  \n",
    "        perturbed_data = attack_fn(model, data, target, **attack_params)\n",
    "        output = model(perturbed_data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        total += target.size(0)\n",
    "        if isinstance(model, LSTMModel) or isinstance(model, TransformerModel):\n",
    "            model.train()\n",
    "    accuracy = correct / total\n",
    "    print(f'Accuracy after adversarial attack: {accuracy * 100:.2f}%')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista modeli do przetestowania\n",
    "models = {\n",
    "    \"SimpleCNN\": SimpleCNN(),\n",
    "    \"LeNet5\": LeNet5(),\n",
    "    \"VGG16\": VGG16(num_classes=10),\n",
    "    \"ResNet18\": ResNet18(num_classes=10),\n",
    "    \"DenseNet121\": DenseNet121(num_classes=10),\n",
    "    \"LSTMModel\": LSTMModel(input_size=28, hidden_size=128, num_layers=2, num_classes=10),\n",
    "    \"TransformerModel\": TransformerModel(input_size=28, num_classes=10, d_model=128, nhead=8, num_layers=2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_datasets = {\n",
    "    \"MNIST\": load_mnist_dataset,\n",
    "    \"FashionMNIST\": load_fashion_mnist_dataset\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacks = {\n",
    "    \"FGSM_1\": (fgsm_attack, {\"epsilon\": 0.3}),\n",
    "    \"FGSM_2\": (fgsm_attack, {\"epsilon\": 0.1}),\n",
    "    \"FGSM_3\": (fgsm_attack, {\"epsilon\": 0.05}),\n",
    "    \"FGSM_4\": (fgsm_attack, {\"epsilon\": 0.01}),\n",
    "    \"PGD_1\": (pgd_attack, {\"epsilon\": 0.3, \"alpha\": 0.01, \"num_iter\": 50}),\n",
    "    \"PGD_2\": (pgd_attack, {\"epsilon\": 0.1, \"alpha\": 0.01, \"num_iter\": 50}),\n",
    "    \"PGD_3\": (pgd_attack, {\"epsilon\": 0.2, \"alpha\": 0.02, \"num_iter\": 100}),\n",
    "    \"PGD_4\": (pgd_attack, {\"epsilon\": 0.05, \"alpha\": 0.005, \"num_iter\": 30}),\n",
    "    \"DeepFool_1\": (deepfool_attack, {\"overshoot\": 0.02, \"max_iter\": 10000}),\n",
    "    \"DeepFool_2\": (deepfool_attack, {\"overshoot\": 0.01, \"max_iter\": 5000}),\n",
    "    \"DeepFool_3\": (deepfool_attack, {\"overshoot\": 0.03, \"max_iter\": 1000}),\n",
    "    \"DeepFool_4\": (deepfool_attack, {\"overshoot\": 0.005, \"max_iter\": 200}),\n",
    "    \"CW_1\": (cw_attack, {\"c\": 1e-3, \"kappa\": 0, \"max_iter\": 1000, \"learning_rate\": 0.01}),\n",
    "    \"CW_2\": (cw_attack, {\"c\": 1e-2, \"kappa\": 0, \"max_iter\": 500, \"learning_rate\": 0.01}),\n",
    "    \"CW_3\": (cw_attack, {\"c\": 1e-4, \"kappa\": 0, \"max_iter\": 2000, \"learning_rate\": 0.005}),\n",
    "    \"CW_4\": (cw_attack, {\"c\": 1e-3, \"kappa\": 0.5, \"max_iter\": 1000, \"learning_rate\": 0.01})\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTesting model: {model_name}\")\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for dataset_name, load_dataset in experimental_datasets.items():\n",
    "        print(f\"\\nTesting dataset: {dataset_name}\")\n",
    "        train_loader, test_loader = load_dataset(transform_needed=(model_name in [\"VGG16\", \"ResNet18\", \"DenseNet121\"]))\n",
    "\n",
    "        start_time = time.time()\n",
    "        train_model(model, 10, train_loader, optimizer, criterion, device, remove_channel_dim=(model_name in [\"LSTMModel\", \"TransformerModel\"]))\n",
    "        training_time = time.time() - start_time\n",
    "\n",
    "        accuracy_before = check_results_before_adversarial_attack(model, test_loader, device)\n",
    "        print(f\"Training time: {training_time:.2f} seconds\")\n",
    "\n",
    "        for attack_name, (attack_fn, attack_params) in attacks.items():\n",
    "            print(f\"\\nTesting attack: {attack_name} with parameters: {attack_params}\")\n",
    "\n",
    "            model_copy = deepcopy(model)\n",
    "            model_copy = model_copy.to(device)\n",
    "\n",
    "            start_time = time.time()\n",
    "            accuracy_after = check_results_after_adversarial_attack(model_copy, test_loader, attack_fn, attack_params, device)\n",
    "            attack_time = time.time() - start_time\n",
    "            print(f\"Attack time: {attack_time:.2f} seconds\")\n",
    "            # print(f\"Accuracy before attack: {accuracy_before * 100:.2f}%\")\n",
    "            # print(f\"Accuracy after attack: {accuracy_after * 100:.2f}%\")\n",
    "\n",
    "            del model_copy, accuracy_after\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        del train_loader, test_loader, accuracy_before"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
